{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1427cf-c883-4c56-8640-5e27adfb2600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameWriter\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c760658-c286-44ad-a590-3849a435e767",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[SecretScope(name='scope31dec')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf6d9db-75f8-4061-88b8-9df4e241e499",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[SecretMetadata(key='blobkeysecret31dec')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.secrets.list(scope = 'scope31dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4595501-cadd-48c2-bfa0-31dc67b02951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.key.healthcareblob31dec.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"scope31dec\", key=\"blobkeysecret31dec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4855ed-28a9-42e0-9bba-75ab0f63b134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/Patient_records.csv</td><td>Patient_records.csv</td><td>5110</td><td>1704010961000</td></tr><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/claims.json</td><td>claims.json</td><td>16385</td><td>1704027744000</td></tr><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/disease.csv</td><td>disease.csv</td><td>1489</td><td>1704010960000</td></tr><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/group.csv</td><td>group.csv</td><td>4390</td><td>1704010960000</td></tr><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/hospital.csv</td><td>hospital.csv</td><td>1328</td><td>1704010960000</td></tr><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/subgroup.csv</td><td>subgroup.csv</td><td>561</td><td>1704010960000</td></tr><tr><td>abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/subscriber.csv</td><td>subscriber.csv</td><td>12061</td><td>1704010960000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/Patient_records.csv",
         "Patient_records.csv",
         5110,
         1704010961000
        ],
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/claims.json",
         "claims.json",
         16385,
         1704027744000
        ],
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/disease.csv",
         "disease.csv",
         1489,
         1704010960000
        ],
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/group.csv",
         "group.csv",
         4390,
         1704010960000
        ],
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/hospital.csv",
         "hospital.csv",
         1328,
         1704010960000
        ],
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/subgroup.csv",
         "subgroup.csv",
         561,
         1704010960000
        ],
        [
         "abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/subscriber.csv",
         "subscriber.csv",
         12061,
         1704010960000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"abfss://rawdata@healthcareblob31dec.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb7df2c-b761-4543-9d4a-3e5704a73d48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgroup_data = spark.read.csv(\"abfss://rawdata@healthcareblob31dec.dfs.core.windows.net/subgroup.csv\", header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3e046a-19b4-4007-9f7a-64bc08de3632",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subgroup_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a4f725-8758-4f42-8024-9659a5f31adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- subgrp_sk: string (nullable = true)\n |-- subgrp_name: string (nullable = true)\n |-- monthly_premium: integer (nullable = true)\n |-- subgrp_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "subgroup_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fd9188-7bda-40f7-aed8-395fa04fadce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subgroup_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e0cc28-04cc-403f-be2c-12b860d30a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+---------+\n|subgrp_sk|subgrp_name|monthly_premium|subgrp_id|\n+---------+-----------+---------------+---------+\n|        0|          0|              0|        0|\n+---------+-----------+---------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "subgroup_data.select([count(when(isnan(c) | col(c).isNull(), c)). alias(c) for c in subgroup_data.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e7617d-49b0-40d1-a29c-5479990a93d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "# drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f44efc-1cf1-44b9-ad36-afeb76b6d131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|subgrp_sk|count|\n+---------+-----+\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "subgroup_data.groupBy('subgrp_sk').count().where(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961ec9d0-daca-4b6f-9e30-f7489ba81305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subgroup_data.groupBy('subgrp_sk', 'subgrp_name', 'monthly_premium', 'subgrp_id').count().where(\"count > 1\").show()\n",
    "# commenting to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46849294-4918-4150-a97e-2cd78507bf56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgroup_data = subgroup_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac88271-4931-4591-9fe3-9daa6f9304fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subgroup_data.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3374195f-fe99-4c87-8f2d-0d26b6039a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgroup_data = subgroup_data.withColumn(\"subgrp_id\", split(subgroup_data['subgrp_id'], \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3d411a-18df-430e-9436-c2090c1b8f87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------------+--------------------+\n|subgrp_sk|        subgrp_name|monthly_premium|           subgrp_id|\n+---------+-------------------+---------------+--------------------+\n|     S104|            Therapy|           1500|[GRP103, GRP113, ...|\n|     S105|          Allergies|           2300|[GRP153, GRP104, ...|\n|     S103|         Physiology|           2000|[GRP122, GRP108, ...|\n|     S102|           Accident|           1000|[GRP110, GRP150, ...|\n|     S101|Deficiency Diseases|           3000|    [GRP101, GRP105]|\n|     S107|             Cancer|           3200|[GRP151, GRP131, ...|\n+---------+-------------------+---------------+--------------------+\nonly showing top 6 rows\n\n"
     ]
    }
   ],
   "source": [
    "subgroup_data.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4208983-6299-4e3e-b724-c9b0e8a1f7c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subgroup_data.show(6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb4bd37-5c6b-46ee-9fd3-232d1ec8e314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode - transforms each element of the list to row\n",
    "subgroup_data = subgroup_data.withColumn(\"subgrp_id\", explode(subgroup_data['subgrp_id']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7a6ddd-7120-4e20-af08-8d89adf05964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subgroup_data.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ff938c-d4da-4eed-ad0e-4568511e72f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#loading into staging data\n",
    "#staging_path = \"abfss://stagingdata@healthcareblob31dec.dfs.core.windows.net\"\n",
    "#subgroup_data.write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").option(\"path\", staging_path).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc730b76-9792-428b-999a-2ed85d5843e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3336075213342467>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mabfss://stagingdata@healthcareblob31dec.dfs.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:205\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    203\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_custom_display_data(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, serialized_plan)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpandas.core.frame\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1507\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1503\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1505\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1506\u001B[0m     )\n",
       "\u001B[0;32m-> 1507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n",
       "\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1557\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1555\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1557\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1558\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1559\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1135\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1129\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1130\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1131\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1132\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1133\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1134\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1135\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1102\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1099\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m-> 1102\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchemaFromList\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1103\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m   1104\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:967\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m    952\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    953\u001B[0m \u001B[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001B[39;00m\n",
       "\u001B[1;32m    954\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    964\u001B[0m \u001B[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001B[39;00m\n",
       "\u001B[1;32m    965\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    966\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
       "\u001B[0;32m--> 967\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema from empty dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    968\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n",
       "\u001B[1;32m    969\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: can not infer schema from empty dataset"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-3336075213342467>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mabfss://stagingdata@healthcareblob31dec.dfs.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:205\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_custom_display_data(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, serialized_plan)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpandas.core.frame\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1507\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1503\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1506\u001B[0m     )\n\u001B[0;32m-> 1507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1557\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1555\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1557\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1558\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1559\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1135\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1129\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1135\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1102\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1099\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m-> 1102\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchemaFromList\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m   1104\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:967\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    952\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    953\u001B[0m \u001B[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001B[39;00m\n\u001B[1;32m    954\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    964\u001B[0m \u001B[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001B[39;00m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    966\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[0;32m--> 967\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema from empty dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    968\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n\u001B[1;32m    969\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n\n\u001B[0;31mValueError\u001B[0m: can not infer schema from empty dataset",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: can not infer schema from empty dataset",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(dbutils.fs.ls(\"abfss://stagingdata@healthcareblob31dec.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d687cb01-538f-4749-bf74-9ccfc1e79445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#subgroup_data.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8a963f-9508-4a0d-91b2-678883bf7aea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_container_path = \"abfss://stagingdata@healthcareblob31dec.dfs.core.windows.net\"\n",
    "output_blob_folder = \"stagingdata/\"\n",
    "subgroup_data.coalesce(1).write.mode(\"overwrite\").option(\"header\" , \"true\"). format(\"com.databricks.spark.csv\").save(output_blob_folder)\n",
    "files = dbutils.fs.ls(output_blob_folder)\n",
    "outputfile = [x for x in files if x.name.startswith(\"part-\")]\n",
    "dbutils.fs.mv(outputfile[0].path, \"%s/subgroup_data_stage.csv\"% output_container_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68cd798d-240f-4b0f-89cb-c8891683e5a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "subgroup - healthcare project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
